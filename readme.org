#+title: DASK Weak scaling evaluation
#+author: Isac Pasianotto


This repository contains code designed to evaluate the weak scaling performance of the [[https://www.dask.org][Dask]] distributed computing library.
The primary purpose of this project is to provide a means to compare the performance of a Dask cluster's installation and find out how different configurations affect the scalability of the system.

The scalability is expressed in terms of [[https://hpc-wiki.info/hpc/Scaling][weak scaling]], which measures the efficiency of a parallel algorithm when the problem size and the number of computational resources are increased proportionally.

In particular the code in this repository was originally developed to compare the performance of a Dask cluster installed on a HPC cluster based on the [[https://slurm.schedmd.com/overview.html][Slurm]] workload manager and the installation on the same hardware but using the [[https://kubernetes.io/][Kubernetes]] container orchestration system.
However the code should be easily adaptable to other distributed computing environments, for more detail see the [[https://docs.dask.org/en/stable/deploying.html][Dask documentation]] related to deploying Dask on different environments.


* Get stared

All the code in this repository was tested using ~python 3.11.9~.

To get started, just clone this repository, create a virtual environment and install the required dependencies:

#+BEGIN_SRC sh
git clone https://github.com/IsacPasianotto/dask-bench
cd dask-bench
python -m venv daskbenchenv
source daskbenchenv/bin/activate
pip install -r requirements.txt
#+END_SRC

And then /remember to adjust the/ [[.env][.env]] file to match your configuration.

*Remark* Slurm uses ~GB~ as the default unit for memory, while Kubernetes uses ~Gi~. Take it into account when setting the memory limits in the ~.env~ file.


** Get starded -  Kubernetes

Despite being a very flexible tools, Dask is also very rigorous in ensuring that the scheduler, the worker and in the case of Kubernetes also the client have the same version of every package installed.

Despite [[https://github.com/dask/dask-docker][official images]] are available, packages in them may not be always alligned with the installed version of Dask in the client.
For this reason it's recommended to build the images locally (maybe pushing it also to a private registry) and then use it.

To do so the provided [[Dockerfile][Dockerfile]] can be used. It's recommended to build the image using the same nodes that will be used to run the benchmark, in order to let the compiler optimize the code for the specific architecture.

#+BEGIN_SRC sh
  source <path_to_virtualenv>/bin/activate
  pip freeze > env.txt
  podman build -t <container_image_name> .
#+END_SRC

It's also highly recommended to push the image to a (private) registry, to let the cluster just pull from it, otherwise the image must be present in every used node.

** Get started - SSL certificates for encrypted communication

In the case of the [[https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html#dask_jobqueue.SLURMCluster][SlurmCluster]] the communication between workers and with the scheduler can be encrypted using SSL certificates.
To do that it's only required to set ~USE_SSL=True~ and provide a valid directory for ~CERT_DIR~  in the [[.env][.env]] file.
The code will automatically generate on the fly the needed self-signed certificates.

In the case of the [[https://kubernetes.dask.org/en/latest/generated/dask_kubernetes.KubeCluster.html][KubeCluster]] some preliminary setup is needed:

  * First of all create a self signed certificates

    #+begin_src
      cd ./certs
      openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=dask-scheduler"
    #+end_src

  * Then, /in the same namespace where the cluster will be deployed/, create a secret with the certificates

    #+begin_src
      source ../.env
      kubectl create secret tls $SECRET_CERT_NAME --key tls.key --cert tls.crt -n $KUBE_NAMESPACE
    #+end_src


* Usage

  * To run the benchmark, just adjust the parameters in the [[.env][.env]] configuration file and run the code with ~python3 main.py~.
  * Note that in the case of the Kubernetes cluster, you must have the ~KUBECONFIG~ environment variable set to the path of the kubeconfig file that allows the client to connect to the Kubernetes cluster.
  * For the Slurm cluster, you can just adjust to your needs the [[./run_example.sh][run_example]] file and submit it with ~sbatch run_example.sh~
  * The code will automatically save all the results in the ~./results~ directory.

* Settings in ~.env~

| name                      | Type   | Description                                                                                                                                                                                                                                                     |
|---------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ~KUBE~                    | bool   | If ~True~, the code will perform the benchmark on a Kubernetes cluster spawning pods among the node. If ~False~, the benchmark will be performed using the SLURMCluster interface to the SLURM resource manager                                                 |
| ~KUBE_NAMESPACE~          | string | The namespace where to spawn the scheduler and the worker pods if ~KUBE~ is True                                                                                                                                                                                |
| ~CONTAINER_IMAGE~         | string | The container image spawned in each pods. It must contains python with all the needed packaged installed. Needed only if ~KUBE~ is True                                                                                                                         |
| ~SCHEDULER_HOST~          | string | Fully qualified domain name of the node in which the scheduler pod will be placed. Needed only if ~KUBE~ is True                                                                                                                                                |
| ~KUBECLUSTER_NAME~        | string | The ~name~ attribute to assign to the created ~DaskCluster~ resource in Kubernetes. Needed only if ~KUBE~ is True                                                                                                                                               |
| ~SCHEDULER_CORES~         | int    | Number of cores to assign to the scheduler pod. Needed only if ~KUBE~ is True                                                                                                                                                                                   |
| ~SCHEDULER_MEM~           | int    | Memory, expressed in ~Gi~ to assign to the scheduler pod. Needed only if ~KUBE~ is True                                                                                                                                                                         |
| ~OUT_DIR~                 | path   | Path to the directory in which put all the output of the slurm job requested for the workers. Needed only if ~KUBE~ is False                                                                                                                                    |
| ~RES_DIR~                 | path   | Path to the directory where to save the results of the performed benchmark                                                                                                                                                                                      |
| ~USE_SSL~                 | bool   | If ~True~, the communication between workers and with the scheduler will be encrypted using SSL                                                                                                                                                                 |
| ~CERT_DIR~                | path   | Used only if ~USE_SSL~ is True. If ~KUBE~ is True is the path to the directory with the self signed certificate to mount as a volume in the spawned pods. Otherwise is the directory where to put the self-signed certificates generated on-the fly by the code |
| ~SECRET_CERT_NAME~        | string | Name of the Kubernetes secrets used to store the self signed certificate. It must be created in the ~KUBE_NAMESPACE~ namespace. Needed only if ~KUBE~ is True                                                                                                   |
| ~RES_FILE_NAME~           | string | Prefix of the file name used to store the final results. It must be a filename without any extension                                                                                                                                                            |
| ~MAX_N_CORES~             | int    | Limit when to stop increasing the number of cores                                                                                                                                                                                                               |
| ~CORES_PER_NODE~          | int    | Total number of cores available in a single node used during the benchmark                                                                                                                                                                                      |
| ~STEP_BY~                 | int    | Starting from ~STEP_BY~ value (except the 1 core case which is performed in any case), then increment the number of core by this value during the benchmark                                                                                                     |
| ~OBS_PER_ITER~            | int    | Number of observation performed with the same number of computational units. Needed to have useful statistics on the observations                                                                                                                               |
| ~MEM_PER_NODE~            | int    | Total amount of RAM available in the nodes used to perform the benchmark. Pay attention that if ~KUBE~ is True, the memory is expressed in ~Gi~, otherwise in ~GB~                                                                                              |
| ~VERBOSE~                 | bool   | If True, the program will print information about the current iteration                                                                                                                                                                                         |
| ~NET_INTERFACE~           | string | Network interface to use for the communication among nodes. Use ~ip link show~ to discover the available options                                                                                                                                                |
| ~ACCOUNT~                 | String | The Slurm account used to ask for resources. Needed only if ~KUBE~ is False                                                                                                                                                                                     |
| ~PARTITION~               | string | The Slurm partition where there are the nodes used for the benchmark. Needed only if ~KUBE~ is False                                                                                                                                                            |
| ~TIME_LIMIT~              | string | Time limit to ask to Slurm for the job. Must be in the format ~hh:mm:ss~. Needed only if ~KUBE~ is False                                                                                                                                                        |
| ~ENV_TO_SOURCE~           | string | Path to the python virtual environment to source to perform the benchmark                                                                                                                                                                                       |
| ~TIMEOUT~                 | int    | Wait this many seconds to the dask cluster to scale before declaring the scaling failed                                                                                                                                                                         |
| ~POLL_INTERVAL~           | int    | Check if the cluster is scaled and ready to accept jobs every this many seconds                                                                                                                                                                                 |
| ~PROBLEM_SIZE_ARRAY~      | int    | Parameter that regulates the workload in the array section of the benchmark                                                                                                                                                                                     |
| ~PROBLEM_SIZE_DATAFRAMES~ | int    | Parameter that regulates the workload in the dataframes section of the benchmark                                                                                                                                                                              |



* Acknowledgements

The code in this repo was heavily inspired by the [[https://github.com/mrocklin/][Matthew Rocklin]]'s [[https://matthewrocklin.com/blog/work/2017/07/03/scaling][blog post]] on this topic.

* Contributing

Contributions are welcome! Feel free to open issues or submit pull requests to improve the functionality and performance of this project.
